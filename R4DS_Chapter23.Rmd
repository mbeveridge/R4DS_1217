---
title: "Chapter23"
output:
  github_document: default
  html_notebook: default
---

```{r}
library(tidyverse)

library(modelr)
options(na.action = na.warn)
```


## 23. "Model basics" [Chapter 18 hardcopy]
### 23.2.1 Exercises

Q1.
One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term.

* Fit a linear model to the simulated data below, and visualise the results.
* Rerun a few times to generate different simulated datasets. What do you notice about the model?

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
```

A1.
From s23.2 R4DS book, "`lm()` has a special way to specify the model family: formulas. Formulas look like `y ~ x`, which `lm()` will translate to a function ... We can fit the model and look at the output" :
```{r}
sim1a_mod <- lm(y ~ x, data = sim1a)               # fit a linear model (straight line)

coef(sim1a_mod)                                    # Slope=1.480507    Intercept=5.806916

# Can't just put `sim1a_mod` into ggplot like below ...It plots *curve*, as for `sim1a`. (WHY?)
ggplot(sim1a_mod, aes(x, y)) +
  geom_point() +
  geom_smooth() 
```


From [https://jrnold.github.io/r4ds-exercise-solutions/model-basics.html#a-simple-model] :
```{r}
ggplot(sim1a, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)           # fit a linear model (straight line)
```


* Ran the model 5 times (ie. reran chunk in the Q, to get different random values, then fitted)

Slope=1.480507    Intercept=5.806916
Slope=1.420044    Intercept=6.886281
Slope=1.569653    Intercept=5.633822
Slope=1.674641    Intercept=5.132300
Slope=1.546082    Intercept=5.633505

The linear model appears to fit fairly well (though we've seen that the line that fits the data best is a curve). Considering the context of the question ('sensitive to unusual values'), yes there were a few 'outliers' that might have had an 'exaggerated' influence on how well the model fit

In [https://jrnold.github.io/r4ds-exercise-solutions/model-basics.html#a-simple-model] this is more apparent (using a function and `facet_wrap()` to plot small multiples, and then showing that changing `rt()` to `rnorm()` generates data that a linear model fits better)


-----------

Q2.
One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance :
```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - make_prediction(mod, data)
  mean(abs(diff))
}
```

Use `optim()` to fit this model to the simulated data above and compare it to the linear model.

A2.
From s23.2 R4DS book, "a numerical minimisation tool called Newton-Raphson search ... In R, we can do that with `optim()`" :
```{r}
best <- optim(c(0, 0), measure_distance, data = sim1a)
best$par                                   # the best parameters of the 'mean-absolute distance'


# HOWEVER, although the code in Q2 defines `measure_distance` (and we have `sim1a` from Q1), that definition doesn't include what the `make_prediction()` function is ...SO at the moment we just get an error
```

From [https://jrnold.github.io/r4ds-exercise-solutions/model-basics.html#a-simple-model]
```{r}
# "For the above function to work, we need to define a function `make_prediction` that takes a numeric vector of length two (the intercept and slope) and returns the predictions..."

make_prediction <- function(mod, data) {
  mod[1] + mod[2] * data$x
}

# (I don't properly understand this, but) running that `make_prediction` code, and then the `optim` code again, gives : [1] 6.013145 1.511948

# ...re. `lm()` with the same data, (in A1) : Slope=1.546082    Intercept=5.633505
```


-----------

Q3.
One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optima. What’s the problem with optimising a three parameter model like this?
```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}
```

A3.
If I understand [https://jrnold.github.io/r4ds-exercise-solutions/model-basics.html#a-simple-model] correctly, it's that although `a[2]` always has the same optimum value, there are many ways of reaching the optimum `a[1] + a[3]` ...and hence many values of `a[1]` and `a[3]` can be part of an optimum. We'll get different solutions depending on which initial values seed `optim()`


-----------

## 23. "Model basics" [Chapter 18 hardcopy]
### 23.3.3 Exercises

Q1.
Instead of using `lm()` to fit a straight line, you can use `loess()` to fit a smooth curve.

* Repeat the process of model fitting, grid generation, predictions, and visualisation on `sim1` using `loess()` instead of `lm()`.
* How does the result compare to `geom_smooth()`?

A1.

* Repeat the process from s23.3.1 (and the end of s23.2 where we obtained `sim1_mod` by using `sim1_mod <- lm(y ~ x, data = sim1)`) ...But use `loess()` instead of `lm()` :

```{r}
sim1_loess <- loess(y ~ x, data = sim1)                # model fitting

grid_loess <- sim1 %>%
  add_predictions(sim1_loess)                          # grid generation, predictions

# "You can also use this function to add predictions [eg. from a `loess` regression] to your original [`sim1`] dataset" -- s23.3.1 R4DS book. (By default, output column gets named `pred`, but here we name it `pred_loess`) :
# sim1 <- sim1 %>%
#  add_predictions(sim1_loess, var = "pred_loess")
```

"Next we plot the [`loess`] predictions". Local Polynomial Regression Fitting (or `loess`) produces a nonlinear, smooth line through the data...

"Fitting is done locally. That is, for the fit at point x, the fit is made using points in a neighbourhood of x, weighted by their distance from x ... For the default family, fitting is by (weighted) least squares" -- Help (`?loess`)


```{r}
ggplot(sim1, aes(x)) +                                 # visualisation
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid_loess, colour = "red", size = 1)
```

* "The predictions of `loess` are the same as the default method for `geom_smooth` because `geom_smooth()` uses `loess()` by default; the message even tells us that" -- [https://jrnold.github.io/r4ds-exercise-solutions/model-basics.html#visualizing-models]


-----------

Q2.
`add_predictions()` is paired with `gather_predictions()` and `spread_predictions()`. How do these three functions differ?

A2.
```{r}
?add_predictions

# "A data frame. `add_prediction` adds a single new column, `.pred`, to the input data. `spread_predictions` adds one column for each model. `gather_predictions` adds two columns `.model` and `.pred`, and repeats the input rows for each model." -- Help

# "Next we add predictions. We’ll use modelr::add_predictions() which takes a data frame and a model. It adds the predictions from the model to a new column in the data frame" -- s23.3.1 R4DS book
```

Based on [https://jrnold.github.io/r4ds-exercise-solutions/model-basics.html#visualizing-models] :

`add_predictions` adds only a single model at a time (so to add two models, you use the function twice). BUT `gather_predictions` and `spread_predictions` allow for adding predictions from multiple models at once.

* `gather_predictions` does it by stacking the results and adding a column with the model name
* `spread_predictions` does it by adding multiple columns (postfixed with the model name) with predictions from each model.

(`spread_predictions` is similar to the example which runs `add_predictions` for each model, and is equivalent to running `spread` after running `gather_predictions`)


-----------

Q3.

* What does `geom_ref_line()` do? What package does it come from?
* Why is displaying a reference line in plots showing residuals useful and important?

A3.
This question relates to code shown in s23.3.2 of R4DS book :
```{r}
ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) +
  geom_point()
# Just showing this for context. (Because running it here gives an error)

?geom_ref_line

# "geom_ref_line {modelr}" "Add a reference line (ggplot2)" -- Help
```

* `geom_ref_line()` adds a reference line to a plot (in **ggplot2**). It comes from **modelr**. "It is equivalent to running `geom_hline` or `geom_vline` with default settings that are useful for visualizing models" -- [https://jrnold.github.io/r4ds-exercise-solutions/model-basics.html#visualizing-models]

* Displaying a reference line (at zero) is useful because "the average of the residual will always be 0" [s23.3.2 R4DS book] and the line (which is thicker than other gridlines) makes it easier to judge whether the model is 'good' ...with "approximately the same variance (or distribution) over the support of x, and no correlation" [https://jrnold.github.io/r4ds-exercise-solutions/model-basics.html#visualizing-models]. More generally, reference lines at other values might direct/maintain a reader's attention on characteristics of the data that show the model *isn't* 'good'. (As with other attention 'hooks', use them with restraint or they overwhelm and lose effectiveness)


-----------

Q4.

* Why might you want to look at a frequency polygon of absolute residuals?
* What are the pros and cons compared to looking at the raw residuals?

A4.
"There are a few different ways to understand what the residuals tell us about the model. One way is to simply draw a frequency polygon to help us understand the spread of the residuals ... This helps you calibrate the quality of the model: how far away are the predictions from the observed values? Note that the average of the residual will always be 0" -- s23.3.2 R4DS book (and a plot of 'raw' residuals is shown there ...ie. +ve & -ve)

"This looks like random noise, suggesting that our model has done a good job of capturing the patterns in the dataset" -- s23.3.2 R4DS book

* You might want to look at a frequency polygon of residuals (absolute or raw) to 'help you calibrate the quality of the model'
* PRO (of absolute) : A clearer/simpler view of 'how far away the predictions are from the observed values', because only one thing (magnitude) is considered and because doubling the count reduces the chance that 'random' patterns are considered significant
* CON : However, using absolute values (combining counts of + & -) might also hide patterns that are really meaningful/systemic ...and which would mean that the model isn't such a good fit


-----------

## 23. "Model basics" [Chapter 18 hardcopy]
### 23.4.5 Exercises

Q1.

* What happens if you repeat the analysis of `sim2` using a model without an intercept.
* What happens to the model equation? What happens to the predictions?

A1.

```{r}

```

-----------

Q2.

* Use `model_matrix()` to explore the equations generated for the models I fit to `sim3` and `sim4`.
* Why is `*` a good shorthand for interaction?

A2.

```{r}

```

*

-----------

Q3.
Using the basic principles, convert the formulas in the following two models into functions.
(Hint: start by converting the categorical variable into 0-1 variables.)
```{r}
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)
```

A3.

```{r}

```

-----------

Q4.

* For `sim4`, which of `mod1` and `mod2` is better? I think `mod2` does a slightly better job at removing patterns, but it’s pretty subtle.
* Can you come up with a plot to support my claim?

A4.

*

```{r}

```


