---
title: "Chapter23"
output:
  github_document: default
  html_notebook: default
---

```{r}
library(tidyverse)
```


## 23. "Model basics" [Chapter 18 hardcopy]
### 23.2.1 Exercises

Q1.
One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term.

* Fit a linear model to the simulated data below, and visualise the results.
* Rerun a few times to generate different simulated datasets. What do you notice about the model?

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
```

A1.
From s23.2 R4DS book, "`lm()` has a special way to specify the model family: formulas. Formulas look like `y ~ x`, which `lm()` will translate to a function ... We can fit the model and look at the output" :
```{r}
sim1a_mod <- lm(y ~ x, data = sim1a)               # fit a linear model (straight line)

coef(sim1a_mod)                                    # Slope=1.480507    Intercept=5.806916

# Can't just put `sim1a_mod` into ggplot like below ...It plots *curve*, as for `sim1a`. (WHY?)
ggplot(sim1a_mod, aes(x, y)) +
  geom_point() +
  geom_smooth() 
```


From [https://jrnold.github.io/r4ds-exercise-solutions/model-basics.html#a-simple-model] :
```{r}
ggplot(sim1a, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)           # fit a linear model (straight line)
```


* Ran the model 5 times (ie. reran chunk in the Q, to get different random values, then fitted)

Slope=1.480507    Intercept=5.806916
Slope=1.420044    Intercept=6.886281
Slope=1.569653    Intercept=5.633822
Slope=1.674641    Intercept=5.132300
Slope=1.546082    Intercept=5.633505

The linear model appears to fit fairly well (though we've seen that the line that fits the data best is a curve). Considering the context of the question ('sensitive to unusual values'), yes there were a few 'outliers' that might have had an 'exaggerated' influence on how well the model fit

In [https://jrnold.github.io/r4ds-exercise-solutions/model-basics.html#a-simple-model] this is more apparent (using a function and `facet_wrap()` to plot small multiples, and then showing that changing `rt()` to `rnorm()` generates data that a linear model fits better)


-----------

Q2.
One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance :
```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - make_prediction(mod, data)
  mean(abs(diff))
}
```

Use `optim()` to fit this model to the simulated data above and compare it to the linear model.

A2.
From s23.2 R4DS book, "a numerical minimisation tool called Newton-Raphson search ... In R, we can do that with `optim()`" :
```{r}
best <- optim(c(0, 0), measure_distance, data = sim1a)
best$par                                   # the best parameters of the 'mean-absolute distance'


# HOWEVER, although the code in Q2 defines `measure_distance` (and we have `sim1a` from Q1), that definition doesn't include what the `make_prediction()` function is ...SO at the moment we just get an error
```

From [https://jrnold.github.io/r4ds-exercise-solutions/model-basics.html#a-simple-model]
```{r}
# "For the above function to work, we need to define a function `make_prediction` that takes a numeric vector of length two (the intercept and slope) and returns the predictions..."

make_prediction <- function(mod, data) {
  mod[1] + mod[2] * data$x
}

# (I don't properly understand this, but) running that `make_prediction` code, and then the `optim` code again, gives : [1] 6.013145 1.511948

# ...re. `lm()` with the same data, (in A1) : Slope=1.546082    Intercept=5.633505
```


-----------

Q3.
One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optima. What’s the problem with optimising a three parameter model like this?
```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}
```

A3.



-----------

## 23. "Model basics" [Chapter 18 hardcopy]
### 23.3.3 Exercises

Q1.
Instead of using `lm()` to fit a straight line, you can use `loess()` to fit a smooth curve.

* Repeat the process of model fitting, grid generation, predictions, and visualisation on `sim1` using `loess()` instead of `lm()`.
* How does the result compare to 'geom_smooth()'?

A1.

```{r}

```

-----------

Q2.
`add_predictions()` is paired with `gather_predictions()` and `spread_predictions()`. How do these three functions differ?

A2.


-----------

Q3.

* What does `geom_ref_line()` do? What package does it come from?
* Why is displaying a reference line in plots showing residuals useful and important?

A3.

*
*

-----------

Q4.

* Why might you want to look at a frequency polygon of absolute residuals?
* What are the pros and cons compared to looking at the raw residuals?

A4.

*
*


-----------

## 23. "Model basics" [Chapter 18 hardcopy]
### 23.4.5 Exercises

Q1.

* What happens if you repeat the analysis of `sim2` using a model without an intercept.
* What happens to the model equation? What happens to the predictions?

A1.

```{r}

```

-----------

Q2.

* Use `model_matrix()` to explore the equations generated for the models I fit to `sim3` and `sim4`.
* Why is `*` a good shorthand for interaction?

A2.

```{r}

```

*

-----------

Q3.
Using the basic principles, convert the formulas in the following two models into functions.
(Hint: start by converting the categorical variable into 0-1 variables.)
```{r}
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)
```

A3.

```{r}

```

-----------

Q4.

* For `sim4`, which of `mod1` and `mod2` is better? I think `mod2` does a slightly better job at removing patterns, but it’s pretty subtle.
* Can you come up with a plot to support my claim?

A4.

*

```{r}

```


